{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "\n",
    "\n",
    "- Extract from database\n",
    "\n",
    "- Accent removal [TODO]\n",
    "- Lowercase\n",
    "- Simple tokenization (word spliting)\n",
    "- Lemmatization (before phrasing, to enhance phrasing)\n",
    "- Bigram/trigram phrase replacement\n",
    "- Remove too-short documents (low 10%)\n",
    "- Stop word removal (after phrasing, to include phrases with stopwords) \n",
    "- Corpus frequency filtering (remove terms present in >50% documents or only present in <0.1% documents)\n",
    "\n",
    "- Generate loadable datasets for `sklearn` and `tomotopy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mezis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mezis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mezis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mezis/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import psycopg2\n",
    "import gensim\n",
    "import sklearn\n",
    "import nltk\n",
    "import re\n",
    "import tomotopy as tp\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "DATASET_NAME = '200k-lemma-nophrase'\n",
    "N_SAMPLES = 200e3\n",
    "DO_LEMMATIZE = True\n",
    "DO_PHRASING = False\n",
    "DO_PHRASE_INCLUDE_SPLIT = True\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to DB...\n",
      "Loading dataset...\n",
      "200000 samples\n",
      "time: 787 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"Connecting to DB...\")\n",
    "conn = psycopg2.connect(\"dbname='peanut_prod' user='mezis'\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "cur.execute(f\"\"\"\n",
    "    SELECT CONCAT_WS(' ', title, body) AS text\n",
    "    FROM post p\n",
    "    JOIN users u ON u.id = p.author_id\n",
    "    WHERE TRUE\n",
    "      AND u.account_status = 'active'\n",
    "      AND p.status = 'active'\n",
    "    ORDER BY p.id DESC\n",
    "    LIMIT {int(N_SAMPLES)}\n",
    "\"\"\")\n",
    "documents_raw = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "print(\"%d samples\" % len(documents_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.88 ms\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS = nltk.corpus.stopwords.words('english') + [\n",
    "    'na' # from tokenizing 'wanna', 'gonna'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:08<00:00, 22974.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = nltk.stem.WordNetLemmatizer()\n",
    "        self.re = re.compile(r\"[a-z0-9']\")\n",
    "        self.map = {\n",
    "            \"'m\":     \"am\",\n",
    "            \"n't\":    \"not\",\n",
    "            \"'d\":     \"would\",\n",
    "            \"'ll\":    \"will\",\n",
    "            \"'ve\":    \"have\"\n",
    "        }\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        tokens = []\n",
    "        for token in nltk.word_tokenize(doc.lower()):\n",
    "            token = self.map.get(token, token)\n",
    "            if DO_LEMMATIZE:\n",
    "                token = self.wnl.lemmatize(token)\n",
    "            if not self.re.match(token): continue\n",
    "            tokens.append(token)\n",
    "        return tokens\n",
    "\n",
    "##################################\n",
    "\n",
    "documents_preprocessed = []\n",
    "tokenizer = MyTokenizer()\n",
    "with multiprocessing.Pool(processes=16) as pool:\n",
    "    for tokenized in pool.map(tokenizer, tqdm(documents_raw)):\n",
    "        documents_preprocessed.append(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:00<00:00, 2264327.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 90.5 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# lengths = np.array([len(doc) for doc in documents_preprocessed])\n",
    "# np.percentile(lengths, 10)\n",
    "# => 8.0\n",
    "\n",
    "documents_filtered = []\n",
    "for doc in tqdm(documents_preprocessed):\n",
    "    if len(doc) < 8: continue\n",
    "    documents_filtered.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182080/182080 [00:12<00:00, 14408.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phrases = gensim.models.phrases.Phrases(\n",
    "    sentences=tqdm(documents_filtered),\n",
    "    min_count=200, # only bigrams with this corpus frequency\n",
    "    threshold=10,  # opaque scoring threshold\n",
    "    common_terms=STOPWORDS,\n",
    "    scoring='default'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182080/182080 [00:20<00:00, 8984.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top bigrams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, (b'braxton hick', 3358.4204775272415)),\n",
       " (1, (b'moses basket', 2086.01979639733)),\n",
       " (2, (b'gestational diabetes', 1532.6169853648114)),\n",
       " (3, (b'universal credit', 1382.1513037461982)),\n",
       " (4, (b'mucus plug', 1199.4010637552137)),\n",
       " (5, (b'stretch mark', 1065.1318046748154)),\n",
       " (6, (b'c section', 991.7321090774194)),\n",
       " (7, (b'sippy cup', 962.5809561152163)),\n",
       " (8, (b'cradle cap', 865.609460458241)),\n",
       " (9, (b'social distancing', 814.8740641785644)),\n",
       " (10, (b'social medium', 687.6258867840921)),\n",
       " (11, (b'potty training', 673.3941719474727)),\n",
       " (12, (b'greatly appreciated', 617.7489274913847)),\n",
       " (13, (b'mental health', 531.368829573375)),\n",
       " (14, (b'growth spurt', 516.793169257245)),\n",
       " (15, (b'tommee tippee', 515.0080911841272)),\n",
       " (16, (b'car seat', 513.0795906653941)),\n",
       " (17, (b'gender reveal', 451.201849235906)),\n",
       " (18, (b'fall asleep', 410.78082391087463)),\n",
       " (19, (b'health visitor', 409.9537516447388)),\n",
       " (20, (b'clear blue', 350.2920426891441)),\n",
       " (21, (b'fertile window', 340.905770690175)),\n",
       " (22, (b'acid reflux', 337.68373191337975)),\n",
       " (23, (b'corona virus', 307.94100995408)),\n",
       " (24, (b'implantation bleeding', 269.87234899404996)),\n",
       " (25, (b'faint line', 269.41344712638016)),\n",
       " (26, (b'trigger shot', 260.84951382171766)),\n",
       " (27, (b'pro and con', 249.99656026249343)),\n",
       " (28, (b'post partum', 238.85996006234174)),\n",
       " (29, (b'toilet paper', 233.56728304077345)),\n",
       " (30, (b'third trimester', 222.12336769192711)),\n",
       " (31, (b'stimulus check', 209.4802672890445)),\n",
       " (32, (b'thanks in advance', 195.55499830179224)),\n",
       " (33, (b'travel system', 185.52644918842748)),\n",
       " (34, (b'covid 19', 183.7911844034245)),\n",
       " (35, (b'panic attack', 175.58764617691153)),\n",
       " (36, (b'belly button', 161.686763209311)),\n",
       " (37, (b'play date', 159.20177613257252)),\n",
       " (38, (b'white noise', 158.77768378126288)),\n",
       " (39, (b'birth control', 158.08858438202213)),\n",
       " (40, (b'morning sickness', 157.3839256709793)),\n",
       " (41, (b'side effect', 151.47029510730232)),\n",
       " (42, (b'high risk', 150.1532385721875)),\n",
       " (43, (b'fast forward', 149.84944795208895)),\n",
       " (44, (b'maternity leave', 148.47629361110907)),\n",
       " (45, (b'finger crossed', 146.58951492280445)),\n",
       " (46, (b'birthday party', 134.627200396383)),\n",
       " (47, (b'hcg level', 130.11935323788433)),\n",
       " (48, (b'weight gain', 120.7682072251442)),\n",
       " (49, (b'mother in law', 114.75642384945392)),\n",
       " (50, (b'gripe water', 110.25291169849419)),\n",
       " (51, (b'hospital bag', 99.33558837806734)),\n",
       " (52, (b'heart beat', 97.74960515468948)),\n",
       " (53, (b'breast pump', 97.48204171528108)),\n",
       " (54, (b'video game', 94.9680893070692)),\n",
       " (55, (b'family member', 92.840798245291)),\n",
       " (56, (b'cow milk', 92.01936572278976)),\n",
       " (57, (b'sleep regression', 91.98556580781549)),\n",
       " (58, (b'story short', 90.31519130130317)),\n",
       " (59, (b'30 min', 89.47485418922224)),\n",
       " (60, (b'blood pressure', 88.56631764778489)),\n",
       " (61, (b'1st birthday', 88.11863613485058)),\n",
       " (62, (b'milk supply', 83.45416216902339)),\n",
       " (63, (b'middle name', 82.7065450061535)),\n",
       " (64, (b'success story', 81.19814748051803)),\n",
       " (65, (b'high chair', 80.57218111913953)),\n",
       " (66, (b'giving birth', 78.64063249576041)),\n",
       " (67, (b'gave birth', 68.3793527821053)),\n",
       " (68, (b'trying to conceive', 67.38589521742455)),\n",
       " (69, (b'even though', 67.26122110556535)),\n",
       " (70, (b'similar situation', 62.77125816636321)),\n",
       " (71, (b'sex drive', 62.32380269659341)),\n",
       " (72, (b'sharp pain', 62.08010617340761)),\n",
       " (73, (b'meet ups', 60.61074558412663)),\n",
       " (74, (b'breast milk', 59.61823692282449)),\n",
       " (75, (b'fell asleep', 57.8104893484831)),\n",
       " (76, (b'weight loss', 56.66157805478889)),\n",
       " (77, (b'falling asleep', 56.10554554861457)),\n",
       " (78, (b'breast feeding', 55.54493534874124)),\n",
       " (79, (b'due date', 55.0498814360984)),\n",
       " (80, (b'ovulation test', 50.55499618151285)),\n",
       " (81, (b'yr old', 46.823284897201475)),\n",
       " (82, (b'30 minute', 46.63961799220447)),\n",
       " (83, (b'hair loss', 45.94585010482623)),\n",
       " (84, (b'looking forward', 45.13772680308959)),\n",
       " (85, (b'antenatal class', 44.74156795149203)),\n",
       " (86, (b'lose weight', 44.69003457798877)),\n",
       " (87, (b'hey lady', 44.43087975487487)),\n",
       " (88, (b'prenatal vitamin', 42.76731937767094)),\n",
       " (89, (b'sore throat', 42.26499550505751)),\n",
       " (90, (b'hi lady', 42.22872117136086)),\n",
       " (91, (b'stay at home', 39.82238947187322)),\n",
       " (92, (b'chemical pregnancy', 39.7979753774163)),\n",
       " (93, (b'make sense', 39.05398180973311)),\n",
       " (94, (b'cloth diaper', 38.94796268134951)),\n",
       " (95, (b'got ta', 38.08351576921726)),\n",
       " (96, (b'anyone else', 37.15092727023432)),\n",
       " (97, (b'trying to figure', 37.07440397967453)),\n",
       " (98, (b'midwife appointment', 37.04032372329013)),\n",
       " (99, (b'living room', 35.15305932555982))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "x = list(phrases.export_phrases(tqdm(documents_filtered)))\n",
    "print('Top bigrams')\n",
    "list(enumerate(sorted(set(x), key=lambda t: -t[1])))[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 955 µs\n"
     ]
    }
   ],
   "source": [
    "if DO_PHRASING:\n",
    "    import re\n",
    "    re_phrase = re.compile('_')\n",
    "    documents_phrased = []\n",
    "    for doc in tqdm(documents_filtered):\n",
    "        tokens = []\n",
    "        for tok in phrases[doc]:\n",
    "            tokens.append(tok)\n",
    "            if DO_PHRASE_INCLUDE_SPLIT and tok.find('_') >= 0:\n",
    "                tokens.extend(tok.split('_'))\n",
    "        documents_phrased.append(tokens)\n",
    "else:\n",
    "    documents_phrased = documents_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182080/182080 [00:15<00:00, 11934.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "documents_destopped = []\n",
    "for doc in tqdm(documents_phrased):\n",
    "    doc = [tok for tok in doc if not tok in STOPWORDS]\n",
    "    documents_destopped.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182080/182080 [00:01<00:00, 143022.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87660 terms\n",
      "Removing 85057 low-frequency terms\n",
      "Removing 0 high-frequency terms:\n",
      "2603 terms in vocabulary\n",
      "time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "# filter terms by CF (>0.1%, <50%)\n",
    "\n",
    "import collections\n",
    "terms = collections.defaultdict(lambda: 0)\n",
    "for doc in tqdm(documents_destopped):\n",
    "    for token in set(doc):\n",
    "        terms[token] += 1\n",
    "\n",
    "print('%d terms' % len(terms))\n",
    "\n",
    "thr_min = len(documents_destopped) * 0.001\n",
    "thr_max = len(documents_destopped) * 0.50\n",
    "\n",
    "terms_low  = [tok for tok, freq in terms.items() if (freq < thr_min)]\n",
    "terms_high = [tok for tok, freq in terms.items() if (freq > thr_max)]\n",
    "\n",
    "print('Removing %d low-frequency terms' % len(terms_low))\n",
    "print('Removing %d high-frequency terms:' % len(terms_high))\n",
    "\n",
    "vocabulary = dict([(tok,freq / len(documents_destopped)) for tok, freq in terms.items() if (thr_min < freq < thr_max)])\n",
    "print('%d terms in vocabulary' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "# save dataset (pickle format, good for SkLearn)\n",
    "\n",
    "import pickle\n",
    "\n",
    "cutoff = int(len(documents_destopped) * (1-TEST_RATIO))\n",
    "\n",
    "pickle.dump({\n",
    "    'train':      documents_destopped[:cutoff],\n",
    "    'test':       documents_destopped[cutoff:],\n",
    "    'vocabulary': vocabulary,\n",
    "}, open(f'dataset.{DATASET_NAME}.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anyone', 48391),\n",
       " ('baby', 45502),\n",
       " ('like', 37060),\n",
       " ('ha', 36683),\n",
       " ('get', 36406),\n",
       " ('wa', 35714),\n",
       " ('week', 35463),\n",
       " ('know', 33799),\n",
       " ('month', 32421),\n",
       " ('day', 31349),\n",
       " ('time', 30948),\n",
       " ('one', 26871),\n",
       " ('want', 25867),\n",
       " ('old', 25341),\n",
       " ('feel', 25146),\n",
       " ('would', 24194),\n",
       " ('go', 22281),\n",
       " ('help', 21307),\n",
       " ('really', 21061),\n",
       " ('need', 20699)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 582 ms\n"
     ]
    }
   ],
   "source": [
    "# top terms\n",
    "sorted(terms.items(), key=lambda t: -t[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
