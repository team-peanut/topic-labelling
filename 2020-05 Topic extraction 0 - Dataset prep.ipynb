{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "\n",
    "\n",
    "- Extract from database\n",
    "\n",
    "- Accent removal [TODO]\n",
    "- Lowercase\n",
    "- Simple tokenization (word spliting)\n",
    "- Lemmatization (before phrasing, to enhance phrasing)\n",
    "- Bigram/trigram phrase replacement\n",
    "- Remove too-short documents (low 10%)\n",
    "- Stop word removal (after phrasing, to include phrases with stopwords) \n",
    "- Corpus frequency filtering (remove terms present in >50% documents or only present in <0.1% documents)\n",
    "\n",
    "- Generate loadable datasets for `sklearn` and `tomotopy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mezis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mezis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mezis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mezis/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "\n",
    "import psycopg2\n",
    "import gensim\n",
    "import sklearn\n",
    "import nltk\n",
    "import re\n",
    "import tomotopy as tp\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "DATASET_NAME = '400k-lemma-nophrase-v2'\n",
    "N_SAMPLES = 400e3\n",
    "DO_LEMMATIZE = True\n",
    "DO_PHRASING = False\n",
    "DO_PHRASE_INCLUDE_SPLIT = True\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "# DATASET_NAME = '200k-lemma-nophrase-v2'\n",
    "# N_SAMPLES = 200e3\n",
    "# DO_LEMMATIZE = True\n",
    "# DO_PHRASING = False\n",
    "# DO_PHRASE_INCLUDE_SPLIT = True\n",
    "# TEST_RATIO = 0.2\n",
    "\n",
    "# DATASET_NAME = '20k-lemma-nophrase-v2'\n",
    "# N_SAMPLES = 20e3\n",
    "# DO_LEMMATIZE = True\n",
    "# DO_PHRASING = False\n",
    "# DO_PHRASE_INCLUDE_SPLIT = True\n",
    "# TEST_RATIO = 0.2\n",
    "\n",
    "# DATASET_NAME = 'small'\n",
    "# N_SAMPLES = 5e3\n",
    "# DO_LEMMATIZE = True\n",
    "# DO_PHRASING = False\n",
    "# DO_PHRASE_INCLUDE_SPLIT = True\n",
    "# TEST_RATIO = 0.2\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 937 µs\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "Doc = collections.namedtuple('Doc', ['id', 'raw', 'tokens'], defaults=(None,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to DB...\n",
      "Loading dataset...\n",
      "400000 samples\n",
      "time: 1.57 s\n"
     ]
    }
   ],
   "source": [
    "print(\"Connecting to DB...\")\n",
    "conn = psycopg2.connect(\"dbname='peanut_prod' user='mezis'\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "cur.execute(f\"\"\"\n",
    "    SELECT p.id, CONCAT_WS(' ', title, body) AS text\n",
    "    FROM post p\n",
    "    JOIN users u ON u.id = p.author_id\n",
    "    WHERE TRUE\n",
    "      AND u.account_status = 'active'\n",
    "      AND p.status = 'active'\n",
    "    ORDER BY p.id DESC\n",
    "    LIMIT {int(N_SAMPLES)}\n",
    "\"\"\")\n",
    "documents_raw = [Doc(id=row[0], raw=row[1]) for row in cur.fetchall()]\n",
    "\n",
    "print(\"%d samples\" % len(documents_raw))\n",
    "documents = documents_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.13 ms\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS = nltk.corpus.stopwords.words('english') + [\n",
    "    'na' # from tokenizing 'wanna', 'gonna'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:19<00:00, 20915.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24.3 s\n"
     ]
    }
   ],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = nltk.stem.WordNetLemmatizer()\n",
    "        self.re = re.compile(r\"[a-z0-9']\")\n",
    "        self.map = {\n",
    "            \"'m\":     \"am\",\n",
    "            \"n't\":    \"not\",\n",
    "            \"'d\":     \"would\",\n",
    "            \"'ll\":    \"will\",\n",
    "            \"'ve\":    \"have\"\n",
    "        }\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        tokens = []\n",
    "        for token in nltk.word_tokenize(doc.raw.lower()):\n",
    "            token = self.map.get(token, token)\n",
    "            # FIXME: Lemmatize using spaCy, only for adv/adv/noun POS?\n",
    "            if DO_LEMMATIZE:\n",
    "                token = self.wnl.lemmatize(token)\n",
    "            if not self.re.match(token): continue\n",
    "            tokens.append(token)\n",
    "        return Doc(raw=doc.raw, id=doc.id, tokens=tokens)\n",
    "\n",
    "##################################\n",
    "\n",
    "documents_preprocessed = []\n",
    "tokenizer = MyTokenizer()\n",
    "with multiprocessing.Pool(processes=16) as pool:\n",
    "    for doc in pool.map(tokenizer, tqdm(documents)):\n",
    "        documents_preprocessed.append(doc)\n",
    "documents = documents_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:26<00:00, 15286.33it/s]\n",
      "100%|██████████| 400000/400000 [00:44<00:00, 8970.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top bigrams\n",
      "[(0, (b'braxton hick', 4898.825918457243)), (1, (b'tommee tippee', 4611.2339690879935)), (2, (b'cradle cap', 4522.206263802449)), (3, (b'moses basket', 3054.1706013986013)), (4, (b'gestational diabetes', 3033.9288217783755)), (5, (b'san diego', 2201.790349936201)), (6, (b'growth spurt', 2152.6255924916936)), (7, (b'universal credit', 2081.0362438220754)), (8, (b'mucus plug', 1862.695465378392)), (9, (b'pro and con', 1693.8517189758413))]\n",
      "time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "phrases = gensim.models.phrases.Phrases(\n",
    "    sentences=map(lambda d: d.tokens, tqdm(documents)),\n",
    "    min_count=200, # only bigrams with this corpus frequency\n",
    "    threshold=10,  # opaque scoring threshold\n",
    "    common_terms=STOPWORDS,\n",
    "    scoring='default'\n",
    ")\n",
    "\n",
    "x = list(phrases.export_phrases(map(lambda d: d.tokens, tqdm(documents))))\n",
    "print('Top bigrams')\n",
    "print(list(enumerate(sorted(set(x), key=lambda t: -t[1])))[:10])\n",
    "\n",
    "if DO_PHRASING:\n",
    "    import re\n",
    "    re_phrase = re.compile('_')\n",
    "    documents_phrased = []\n",
    "    for doc in tqdm(documents):\n",
    "        tokens = []\n",
    "        for tok in phrases[doc.tokens]:\n",
    "            tokens.append(tok)\n",
    "            if DO_PHRASE_INCLUDE_SPLIT and tok.find('_') >= 0:\n",
    "                tokens.extend(tok.split('_'))\n",
    "        documents_phrased.append(Doc(raw=doc.raw, id=doc.id, tokens=tokens))\n",
    "else:\n",
    "    documents_phrased = documents\n",
    "\n",
    "documents = documents_phrased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:05<00:00, 68282.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.86 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "documents_destopped = []\n",
    "sw = set(STOPWORDS)\n",
    "for doc in tqdm(documents):\n",
    "    tokens = [tok for tok in doc.tokens if not tok in sw]\n",
    "    documents_destopped.append(Doc(tokens=tokens, raw=doc.raw, id=doc.id))\n",
    "\n",
    "documents = documents_destopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:02<00:00, 144249.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142233 terms\n",
      "Removing 139798 low-frequency terms\n",
      "Removing 0 high-frequency terms:\n",
      "2430 terms in vocabulary\n",
      "time: 6.68 s\n"
     ]
    }
   ],
   "source": [
    "# filter terms by CF (>0.1%, <50%)\n",
    "\n",
    "import collections\n",
    "terms = collections.defaultdict(lambda: 0)\n",
    "for doc in tqdm(documents):\n",
    "    for token in set(doc.tokens):\n",
    "        terms[token] += 1\n",
    "\n",
    "print('%d terms' % len(terms))\n",
    "\n",
    "thr_min = len(documents) * 0.001\n",
    "thr_max = len(documents) * 0.50\n",
    "\n",
    "terms_low  = [tok for tok, freq in terms.items() if (freq < thr_min)]\n",
    "terms_high = [tok for tok, freq in terms.items() if (freq > thr_max)]\n",
    "\n",
    "print('Removing %d low-frequency terms' % len(terms_low))\n",
    "print('Removing %d high-frequency terms:' % len(terms_high))\n",
    "\n",
    "vocabulary = dict([(tok,freq / len(documents)) for tok, freq in terms.items() if (thr_min < freq < thr_max)])\n",
    "print('%d terms in vocabulary' % len(vocabulary))\n",
    "\n",
    "documents_filtered_cf = []\n",
    "vocab_set = set(vocabulary.keys())\n",
    "for doc in documents:\n",
    "    tokens = [term for term in doc.tokens if term in vocab_set]\n",
    "    documents_filtered_cf.append(Doc(tokens=tokens, id=doc.id, raw=doc.raw))\n",
    "\n",
    "documents = documents_filtered_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:00<00:00, 1667867.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retaining 366823 documents\n",
      "time: 244 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# lengths = np.array([len(doc) for doc in documents])\n",
    "# np.percentile(lengths, 10)\n",
    "# => 4.0\n",
    "\n",
    "documents_filtered = []\n",
    "for doc in tqdm(documents):\n",
    "    if len(doc.tokens) < 4: continue\n",
    "    documents_filtered.append(doc)\n",
    "print(f'Retaining {len(documents_filtered)} documents')\n",
    "documents = documents_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.09 s\n"
     ]
    }
   ],
   "source": [
    "# save dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "cutoff = int(len(documents) * (1-TEST_RATIO))\n",
    "to_tokens = lambda d: d.tokens\n",
    "to_ids    = lambda d: d.id\n",
    "to_raw    = lambda d: d.raw\n",
    "\n",
    "pickle.dump({\n",
    "    'raw': {\n",
    "      'train': list(map(to_raw, documents[:cutoff])),\n",
    "      'test':  list(map(to_raw, documents[cutoff:])),\n",
    "    },\n",
    "    'ids': {\n",
    "      'train': list(map(to_ids, documents[:cutoff])),\n",
    "      'test':  list(map(to_ids, documents[cutoff:])),\n",
    "    },\n",
    "    'tokenised': {\n",
    "      'train': list(map(to_tokens, documents[:cutoff])),\n",
    "      'test':  list(map(to_tokens, documents[cutoff:])),\n",
    "    },\n",
    "    'vocabulary': vocabulary,\n",
    "}, open(f'dataset.{DATASET_NAME}.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
